{
  
    
        "post0": {
            "title": "Making your Deep RL matters.",
            "content": "A collection of implementation tricks, hyperparameter sensitivity, and others in Deep RL which I gave a presentation in my research group. . Author: Long M. Luu, contact: minhlong9413@gmail.com or Discord AerysS#5558. . “Your ResNet, batchnorm, and very deep networks don’t work here.” - Andrej Karpathy . . References . All codebases are released. Just use CatalyzeX. . Deep Reinforcement Learning that Matters . How Many Random Seeds? Statistical Power Analysis in Deep Reinforcement Learning Experiments . Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control . Implementation Matters in Deep RL: A Case Study on PPO and TRPO . Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control . What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale… . An Empirical Model of Large-Batch Training . Deep Reinforcement Learning at the Edge of the Statistical Precipice . http://joschu.net/docs/nuts-and-bolts.pdf . http://amid.fish/reproducing-deep-rl . https://www.alexirpan.com/2018/02/14/rl-hard.html . https://openai.com/blog/science-of-ai/ . https://costa.sh/blog-the-32-implementation-details-of-ppo.html . Why it matters? . Reproducibility, especially in Deep RL, is hard (sources above). | Multiple factors affect the results: random seed, hyperparameters, code-level optimizations. | It is common to report the best of N results, which makes misleading claims. | . . Agarwal et al., 2021 . Statistical Power Analysis (Colas et al., 2017 paper) . Consider two algorithms below. The name of the algorithm is not important. The mean and 95% confidence interval are averaged over 5 seeds. Our concern: is algorithm 1 better than 2? . . The measure of performance: the average cumulated reward over last 100 evaluation episodes. It seems like Algo 1 is better than Algo 2. . Statistical problem . The performance can be modeled as a random variable $X$. Running this algorithm results in $x^i$. Run for N times, we obtain a statistical sample $x = (x^1,…,x^N)$. . A random variable is characterized by its mean $ mu$ and standard deviation $ sigma$. The real values are unknown, so we compute the unbiased estimations $ overline{x}= sum_{i=1}^n x^i$ and $s approx sqrt{ frac{ sum_{i+1}^N (x^i - overline{x})^2}{N-1}}$. The larger the $N$, the more confident one can be in the estimations. . Here, two algorithms with respective performances $X_1$ and $X_2$ are compared. If they follow normal distributions, then $X_{ text{diff}} = X_1 - X_2$ also follows a normal distribution with parameters $ sigma_{ text{diff}} = sqrt{( sigma_1^2 + sigma_2^2)}$ and $ mu_{ text{diff}} = mu_1 - mu_2$. . In this case, the estimator of the mean of $X_{ text{diff}}$ is $ overline{x}{ text{diff}} = overline{x}_1 - overline{x_2}$ and the estimator of $ sigma{ text{diff}}$ is $s_{ text{diff}} = sqrt{s_1^2 + s_2^2}$. The effect size $ epsilon$ can be defined as $ epsilon = mu_1 - mu_2$. . Testing $ epsilon$ between two algorithms is mathematically equivalent to testing a difference between $ mu_{ text{diff}}$ and 0. . Difference test . We define the null hypothesis $H_0$and the alternate hypothesis $H_a$ using the two-tail case: . $H_0: mu_{ text{diff}} = 0$ | $H_a: mu_{ text{diff}} neq 0$ | . When we have an assumption about which algorithm performs better (say, Algo 1), we can use the one-tail version: . $H_0: mu_{ text{diff}} leq 0$ | $H_a: mu_{ text{diff}} &gt; 0$ | . At first, we assume the null hypothesis. Once a sample $x_{ text{diff}}$ is sampled from $X_{ text{diff}}$, we can estimate the probability $p$ (called $p$-value) of observing the data as *extreme (*$ overline{x}_{ text{diff}}$ is far from 0), under the null hypothesis assumption. $p$-value answers the question: . How probable is it to observe this sample or a more extreme one, given that there is no true difference in the performances of both algorithms? . We can rewrite it for the one-tail case: . p-value=P(Xdiff≥x‾diff∣H0)p text{-value} = P(X_{ text{diff}} geq overline{x}_{ text{diff}} | H_0)p-value=P(Xdiff​≥xdiff​∣H0​) . For the two-tail case: . p-value={P(Xdiff≥x‾diff∣H0),x‾diff&gt;0P(Xdiff≤x‾diff∣H0),x‾diff≤0p text{-value} = begin{cases} &amp; P(X_{ text{diff}} geq overline{x}_{ text{diff}} | H_0), overline{x}_{ text{diff}} &gt; 0 &amp; P(X_{ text{diff}} leq overline{x}_{ text{diff}} | H_0), overline{x}_{ text{diff}} leq 0 end{cases}p-value={​P(Xdiff​≥xdiff​∣H0​),xdiff​&gt;0P(Xdiff​≤xdiff​∣H0​),xdiff​≤0​ . When this probability becomes really low, it means that it is highly improbable that two algorithms with no performance difference produced the collected (Algo 1 is different from Algo 2). . A difference is called significant at level $ alpha$ when $p$-value &lt; $ alpha$ in the one-tail case, and $ alpha/2$ in the two-tail case. Usually $ alpha=0.05$. However, $ alpha = 0.05$ also means there is a 5% chance we conclude it wrong. . In the paper by Colas et al., the authors also suggest an alternate way to test the difference using 95% confidence intervals (95% CIs). However, I will only focus on the t-test for now. . Statistical testing . . Type-I error: false positive. Rejects $H_0$ when it is true. . Type-II error: false negative. Accepts $H_0$ when it is false. . The t-test and Welch’s t-test . The t-test assumes that the variances of both algorithms are equal, while Welch’s t-test assumes they are unequal. Both tests are equivalent when the std are equal. . The t-test assumes the following: . The scale of data measurements must be continuous and ordinal (can be ranked). This is the case in RL. | Data is obtained by collecting a representative sample from the population. This seem reasonable in RL. | Measurements are independent from one another. This seems reasonable in RL. | Data is normally-distributed, or at least bell-shaped. | . We then compute the $t$-statistic and the degree of freedom $ nu$ using the following equations: . . where $x_{ text{diff}} = x_1 - x_2$; $s_1, s_2$ is the empirical standard deviations of the two samples and $N_1, N_2$ are their sizes (which we assume $N_1 = N_2 = N$). . A figure to make sense of these concepts: . . $H_0$ assumes $ mu_{ text{diff}} = 0$, so the distribution is centered on 0. $H_a$ assumes a positive difference $ mu_{ text{diff}} = epsilon$, so the distribution is shifted by the t-value corresponding to $ epsilon$, $t_ epsilon$. We consider the one-tail case, and test for the positive difference. . Using the computed t-statistic and $ nu$, we can compute the $ alpha$ value. $ alpha$ is enough to declare statistical significance. With modern software work, we can directly compute t-statistic and $ alpha$ without worrying about $ nu$. . Back to the problem . . The measure of performance: the average cumulated reward over last 100 evaluation episodes. It seems like Algo 1 is better than Algo 2. The p-value returned is 0.031, which is lower than $ alpha = 0.05$. . However, they are the same algorithm: DDPG. They have the same set of parameters, are evaluated on the same environment (so there is no implementation tricks involved), and are averaged over 5 seeds each. . Estimate the type-I error. . Experiment: same algorithm, runs for $N = [2, 21]$. . . So in practice, $N={5, 10}$ works well. . Deep Reinforcement Learning that matters . Empirical results from Henderson et al., 2017 paper. . Reward Scaling . Idea: Multiply the reward by some scalar: $r = sigma r$. | Why it matters: this affects action-value function based method like DDPG. | . . . Random seeds and trials . Idea: run multiple runs with different random seeds | Why it matters: environment stochasticity or stochasticity in the learning process, e.g. random weight initialization, Q-value initialization. | . . Additional result from Islam et al., 2017 paper: . . . Environment variables . Idea: different environment can affect the performance | Why it matters: although the reward can be high, it can learn undesirable policy. | . In this figure, HalfCheetah has stable dynamics. Hopper does not have stable dynamics. Swimmer has a local optima. . . https://www.youtube.com/watch?v=lKpUQYjgm80 . https://www.youtube.com/watch?v=ghCo7ERx6qo . CoastRunners does not directly reward the player’s progression around the course, instead the player earns higher scores by hitting targets laid out along the route. We assumed the score the player earned would reflect the informal goal of finishing the race, so we included the game in an internal benchmark designed to measure the performance of reinforcement learning systems on racing games. However, it turned out that the targets were laid out in such a way that the reinforcement learning agent could gain a high score without having to finish the course. This led to some unexpected behavior when we trained an RL agent to play the game. . https://www.youtube.com/watch?v=tlOIHko8ySg&amp;t=1s . Implementation tricks . Idea: code-level optimization tricks like advantage normalization, n-steps TD return. | Why it matters: it drastically changes the result. | . Consider: . Set 1: TRPO from TRPO codebase (Schulman 2015), from PPO codebase(Schulman 2017), and rllib Tensorflow (Duan 2016) codebases. | Set 2: DDPG rllab Theano (Duan 2016), rllabplusplus (Gu 2016), OpenAI Baselines (Plapper 2017). | . . . Implementation Matters in Deep RL (Engstorm et al., 2020 paper) . Different code-level optimization tricks can lead to (dramatically) different results. . Specifically, PPO implementation contains these optimizations that are not (or barely) described in the original paper: . Value function clipping. PPO originally suggests fitting the value network via regression to target values: $L^V = (V_{ theta_t} - V_{targ})^2$. However, the implementation in OpenAI Baselines fits the network with a PPO-like objective: | LV=max⁡[(Vθt−Vtarg)2,(clip(Vθt,Vθt−1−ϵ,Vθt−1+ϵ)−Vtarg)2]L^V = max left[ (V_{ theta_t} - V_{targ})^2, (clip(V_{ theta_t}, V_{ theta_{t-1}} - epsilon, V_{ theta_{t-1}} + epsilon) - V_{targ})^2 right]LV=max[(Vθt​​−Vtarg​)2,(clip(Vθt​​,Vθt−1​​−ϵ,Vθt−1​​+ϵ)−Vtarg​)2] . Reward scaling. Rewards are divided through by the std of a rolling discounted sum of the rewards. | Orthogonal initialization and layer scaling. | Adam learning rate annealing. | Reward clipping: [-5, 5] or [-10, 10]. | Observation normalization: states are normalized to mean-zero, variance-one vectors before training. | Observation clipping: like reward. | Hyperbolic tan (tanh) activations. | Global gradient clipping: global $ ell_2$-norm less than 0.5. | . The authors then consider a PPO-M (minimal) variant, that does not use these optimization tricks, alongside PPO and TRPO, and the TRPO+ variant that uses PPO tricks. . . Results comparing 4 algorithms . Define: . . AAI measures the maximal effect of switching algorithms, and ACLI measures the maximal effect of adding tricks. . We have: . . How to tune hyperparameters? . Note: this tuning guide depends on empirical results, not theoretical. . Network Architecture . From Henderson et al., 2017 paper: . Investigate three common architectures: (64, 64), (100, 50, 25) and (400, 30), activation: tanh, ReLU, Leaky ReLU. . . . . From Islam et al., 2017 paper: (purpose: to reproduce results) . . . . . Batch Size . From Islam et al., 2017 paper . . . . Findings from Andrychowicz et al., 2021 paper . Train 250k agents in 5 continuous control environments. Each choice is run for 3 random seeds, but the reported results are based on the performance of hundreds of runs. . . . . . . . . . . . . . . . . . . Suggested method to compare algorithms . Pseudocode thanks to James MacGlashan (Senior Research Scientist - Sony AI). . trials = 10 neval = 20 alg1_performance = [] for _ in range(trials): p = train_alg1(10000) # train for 10,000 time steps, get policy eval_performances = [] for _ in range(neval): episode = run_episode(p) avg_episode_reward = compute_avg_reward(episode) eval_performances.append(avg_episode_reward) trial_performance = np.mean(eval_performances) alg1_performance.append(trial_performance) alg2_performance = [] for _ in range(trials): p = train_alg2(10000) # train for 10,000 time steps, get policy eval_performances = [] for _ in range(neval): episode = run_episode(p) avg_reward = compute_avg_reward(episode) eval_performances.append(avg_reward) trial_performance = np.mean(eval_performances) alg2_performance.append(trial_performance) p_value = t_test(alg1_performance, alg2_performance) . Suggestions and Conclusion . In general, however, the most important step to reproducibility is to report all hyperparameters, implementation details, experimental setup, and evaluation methods for both baseline comparison methods and novel work. Without the publication of implementations and related details, wasted effort on reproducing state-of-the-art works will plague the community and slow down progress. . . Overall, our results highlight the necessity of designing deep RL methods in a modular manner. When building algorithms, we should understand precisely how each component impacts agent training—both in terms of overall performance and underlying algorithmic behavior. It is impossible to properly attribute successes and failures in the complicated systems that make up deep RL methods without such diligence. More broadly, our findings suggest that developing an RL toolkit will require moving beyond the current benchmark-driven evaluation model to a more fine-grained understanding of deep RL methods. . Note: Stable-Baselines3 has all these recommendations implemented by default. .",
            "url": "https://minhlong94.github.io/blog/drl/2021/09/25/Making-your-Deep-RL-matters.html",
            "relUrl": "/drl/2021/09/25/Making-your-Deep-RL-matters.html",
            "date": " • Sep 25, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Action Space Shaping in Deep RL",
            "content": "This is a presentation of the paper “Action space shaping in Deep Reinforcement Learning”, by Anssi Kanervisto et al., 2020, in IEEE Conference on Games 2020. . Author: Long M. Luu. Contact: minhlong9413@gmail.com or AerysS#5558. . Reference . Action Space Shaping in Deep Reinforcement Learning . Introduction . Take a game that uses keyboard and mouse: . Too many keys | Mouse is continuous | . Probably hard for human to learn. Should we remove keys? If we remove keys so that it is still playable, should we also remove unnecessary actions? . . The question: do these transformations support the training of RL agents? . Environments in this paper: toy environment, Atari, VizDoom, Starcraft II and Obstacle Tower. . . . . ![https://miro.medium.com/max/1000/1zkH1_KoHxQwr25thSJFUEQ.gif](https://miro.medium.com/max/1000/1zkH1_KoHxQwr25thSJFUEQ.gif) . Action space shaping . Types of action spaces . There are three common types of actions, established by OpenAI Gym: . Discrete . Each action is an integer $a in {0, 1, …, N}$ where $n in mathbb{N}$ represents the number of possible actions. | . . MultiDiscrete . An extension of Discrete. Action is a vector of individual discrete actions $a_i in {0, 1, …, N_i}$, each with possibly different number of possibilities $N_i$. For example, a keyboard is a large MultiDiscrete space. | . . Continuous. Action $a in mathbb{R}$ is a real number/vector. | . . A set of keyboard buttons and mouse control can be represented as a combination of MultiDiscrete and Continuous. . MultiDiscrete are often treated as independent Discrete decisions. Support for Continuous is often harder to implement correctly than for Discrete space. . Action space shaping in video games . There are three major categories of action space transformation: . RA: Remove actions. For example, “Sneak” in Minecraft is not crucial for the game progress ⇒ often removed. RA helps with exploration since there are less actions to try. However, this requires domain knowledge, and may restrict agent’s capabilities. | . . DC: Discretize continuous actions. Mouse movement or camera turning speed are often discretized by splitting them into a set of bins, or defining as discrete choices: negative, zero, positive. This turning rate is a hyperparameter. If the rate is too high, actions are not fine-grained, so the agents may have difficulties in aiming at a specific spot. | . . CMD : Convert MultiDiscrete to Discrete. Assumption: it is easier to learn a single large policy than multiple small policies. For example Q-Learning only works for Discrete actions. | . . Action spaces of other games . . Experiments . Environments: Atari, VizDoom, Starcraft II, Obstacle Tower challenge. . Algorithm: PPO, IMPALA . Libraries: stable-baselines, rllib. . 8 parallel environments. . Get-To-Goal . A simple reach-the-goal env: player and goal start at a random environment. Player tries to reach the goal (reward 1) or when env times out (reward 0). Agent receives a 2D vector pointing towards the goal, and the rotating angle $(cos( phi), sin( phi))$ tuple where $ phi in [0, 2 pi]$. Goal: test DC by using discrete and continuous variants of the action space: . MultiDiscrete: four buttons Up, Down, Left, Right. | Discrete: flatten version of above, but only one button at a time, i.e. no diagonal movements. | Continuous: player specifies the exact direction of the next move with a continuous value with 0 representing straight up, 90 straight right and 180 straight down. | Tank, Discrete/MultiDiscrete: player has a heading $ phi$ and it can choose to increase/decrease it (turn left/right) and/or to move forward/backward towards the heading. For tank-like controls, player must stop completely to turn. | . Authors also add bogus action (actions that do nothing), and backward and strafe actions. . . Strafe action. Camera of blue is locked towards red. . To study RA and CMD, the authors add and/or remove additional actions. . . . Figure (left): Tank-like control is slower than non-tank. Continuous is slowest. With rllib, observed similar results, except Continuous learned faster than tank-like ⇒ Continuous are sensitive to the implementation. . Figure (right) with and without additional actions ⇒ Agent learns slower the more actions they have (RA). . . Figure shows the agent learns faster on MultiDiscrete spaces ⇒ RL agents can profit from MultiDiscrete compared to Discrete. (CMD). . Atari games . Atari games use Discrete spaces, which consists of only necessary actions to play the game (minimal, default in Gym). Authors add more actions: full, and multi-discrete, where joystick and fire-button are additional buttons with 9 and 2 options respectively. . . Figure 3 shows no clear difference, except: . MsPacman. multi-discrete achieved almost one-quarter higher score. | Enduro: minimal underperforms, despite the fact that the full space does not offer any new actions. | . RA can limit performance, but overall does not change results. Same thing applies to CMD. . VizDoom . In increasing difficulty: . Get-to-goal: Similar to earlier, except it is a first-person shooter scenario. +1 if reaches, 0 otherwise, include one minute of game-time timeout. | HGS: gather health kits to survive | Deathmatch: fight against randomly spawn enemies. +1 per kill, one shot. | . RA scenarios: . Bare-minimum: moving forward, turning left, attack (deathmatch). | Minimal: bare-minimum + turning right. | Backward: minimal + moving backward. | Strafe: backward + moving left and right. | . Five different spaces for each set: . Original MultiDiscrete | Three levels of CMD. | Continuous mouse control DC | . Observation: grayscale (Get-to-goal and HGS), RGB (Deathmatch) of size 80x60 + game variables. . . . . From figure, MultiDiscrete performs as well as discretized version (CMD). Continuous action prevents learning in most spaces (DC). Increasing the number of actions improves the results in difficult cases (RA). . Obstacle Tower . 3D platform game with randomly generated levels. Original space is MultiDiscrete with options to move forward/backward and left/right, turn left/right and jump. . To test CMD and RA, authors disabled strafing, moving backward or forcing moving forward. . Discrete is obtained by creating all possible combinations of MultiDiscrete. . Observation: 84x84 RGB image. . . . From figure, no significant difference between two sets, except Backward action shows slower learning than the rest ⇒ Intuition to remove unnecessary actions. . Starcraft II . From figure Action masking is crucial in BM and CMAG. On BM, RA can lead to significant improvement. . . . Conclusion . Removing actions RA can lower the overall performance (VizDoom) but can be an important step to make environments learnable (SC2, Obstacle Tower). . Continuous are harder to learn than discrete and can also prevent learning. Discretizing them DC improves performance notably. . In Get-To-Goal, MultiDiscrete scales well with an increasing number of actions, while Discrete does not. There is no significant different in other environments. . In short: use MultiDiscrete &gt; Discrete &gt; Continuous. . Start by removing all but the necessary actions and discretizing all continuous actions. Avoid turning multi-discrete actions into a single discrete action and limit the number of choices per discrete action. If the agent is able to learn, start adding removed actions for improved performance, if necessary. . Acknowledgements . Thanks Anssi Kanervisto for reviewing this document. .",
            "url": "https://minhlong94.github.io/blog/drl/2021/09/25/Action-space-shaping-in-Deep-Reinforcement-Learning.html",
            "relUrl": "/drl/2021/09/25/Action-space-shaping-in-Deep-Reinforcement-Learning.html",
            "date": " • Sep 25, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "A Neural Network to print from 1 to 50",
            "content": "Recently, a user named logo asked a question on the RL Discord server: . how to print numbers from 1 to 50 in python? . Little did I know, I was about to engage in one of the funniest conversation. . Pure Python approach . for i in range(50): print(i) . A very simple and straightforward solution. Wait, why can’t we just print(1,2,3,4,5,6,7,8,9,10...,50)? . “That’s too long” - user Ariel. “it’s better to do exec(f&quot;print({&#39;,&#39;.join(str(i) for i in range(1,51))})&quot;)” . Ok but? . i = 1 while True: print(i) i += 1 if i &gt; 50: break . Best method. Straight forward: . print(1) print(2) print(3) . . . . print(50) . “next research question is how to do it with functional approach”? Oh right. . def getprint(): return print getprint()(1) getprint()(2) . . . . getprint()(50) . Or better: [print(i) for i in range(1,51)] . User James replied to the functional approach: . def recursive_print(start, end): def recursive_count_str(i): return f&quot;{i} n{recursive_count_str(i+1)}&quot; if i &lt; end else f&quot;{i}&quot; print(recursive_count_str(start)) recursive_print(1, 50) . Ok, can we do better? . class PrintFactory: def __init__(self, number): self.number = number def print_number(self): for i in range(self.number): print(i) def print50(): print_factory = PrintFactory(50) print_factory.print_number() print50() . by OrganicPitaChips. . “I feel a strong yin yang struggle right now between my shitposting nature and the whole “admin” thing” - Ariel. . class ObjectOrientedPrintFactory: def __init__(self, number): self.number = number self.child_factory = None if number &gt; 0: self.child_factory = ObjectOrientedPrintFactory(number - 1) def print_number(self): if self.child_factory: self.child_factory.print_number() print(self.number) def print50(): print_factory = ObjectOrientedPrintFactory(50) print_factory.print_number() print50() . by hackeronsteriods. . Yeah this is from this link . Neural Network to print from 1 to 50 . “I like how nobody actually suggested training a neural network” - Ariel . “Challenge accepted” - James . import tensorflow as tf import numpy as np class IncrementAutoregressive(tf.keras.layers.Layer): def __init__(self): super().__init__() self._inc_weight = tf.Variable(np.random.normal()) self.terminal_logits_layer = tf.keras.layers.Dense(2) def call(self, inputs, **kwargs): new_inputs = inputs + self._inc_weight terminal_logits = self.terminal_logits_layer(new_inputs) return new_inputs, terminal_logits def rollout(self, start_val): next_num = tf.constant([[start_val]], dtype=tf.float32) stop = tf.constant(False) result = [next_num] while not stop: next_num, stop_logits = self(next_num) stop = tf.reduce_all(tf.argmax(stop_logits, axis=-1) == 1) result.append(next_num) return tf.concat(result, axis=0) def main() -&gt; None: layer = IncrementAutoregressive() xs = tf.reshape(tf.range(1, 50, dtype=tf.float32), (49, 1)) ys = xs + 1.0 stops = tf.concat([tf.fill((48,), 0), [1]], axis=0) opt = tf.keras.optimizers.Adam(0.01) loss1 = tf.keras.losses.MeanSquaredError() loss2 = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) @tf.function def step(): with tf.GradientTape() as tape: pred, pred_stops = layer(xs) pred_loss = loss1(ys, pred) pred_stops_loss = loss2(stops, pred_stops) total_loss = pred_loss + pred_stops_loss grad = tape.gradient(total_loss, layer.trainable_weights) opt.apply_gradients(zip(grad, layer.trainable_weights)) return total_loss for _ in range(10000): loss = step() print(loss) rollout = layer.rollout(1.0) print(tf.round(rollout)) if __name__ == &quot;__main__&quot;: main() . “Please add TPU support I need this to support big data”. . “100k epochs ensures the right output”. . “When I run it locally I usually get 1-50, and somtimes 1-51. Dat bias term learning is apparently rough”. . “but what if I want to use Reinf Learning, we would surely need a PrintGymEnv”? . OK. . import numpy as np import gym from gym.spaces import Box class PrintEnv(gym.Env): def __init__(self, start: int = 1, end: int = 50): self.start = start self.end = end self.counter = start self.observation_space = Box(1, 50, (1,), dtype=np.int32) self.action_space = Box(1, 50, (1,), dtype=np.int32) def reset(self): self.counter = self.start return self.counter def step(self, action: int): if action == self.counter: reward = 1. self.counter += 1 else: reward = 0. if self.counter &gt;= self.end: done = True else: done = False return self.counter, reward, done, {} def render(self, mode=&quot;human&quot;): print(self.counter) . by Ariel. . I think we need a PyTorch version, no? . import torch from torch import nn, Tensor class IncrementAutoregressive(nn.Module): def __init__(self): super().__init__() self.inc_weight = nn.Parameter(torch.normal(0., 1., (1,), requires_grad=True)) self.terminal_logits_layer = nn.Linear(1, 2) def forward(self, inputs): new_inputs = inputs + self.inc_weight terminal_logits = self.terminal_logits_layer(new_inputs) return new_inputs, terminal_logits def rollout(model: IncrementAutoregressive, start_val: int) -&gt; Tensor: next_num = torch.tensor([start_val]) stop = False result = [next_num] i = 0 while not stop: next_num, stop_logits = layer(next_num) stop = stop_logits.argmax().item() result.append(next_num.detach()) i += 1 return torch.cat(result).round() def train(layer: nn.Module): xs = torch.arange(1, 50).view((49, 1)).to(torch.float32) ys = xs + 1.0 stops = torch.zeros_like(xs).view((49,)).to(torch.long) stops[-1] = 1. opt = torch.optim.Adam(layer.parameters(), 0.05) val_loss = nn.MSELoss() stop_loss = nn.CrossEntropyLoss() for t in range(10000): pred, pred_stops = layer(xs) loss = val_loss(pred, ys) + stop_loss(pred_stops, stops) opt.zero_grad() loss.backward() opt.step() if t % 1000 == 0: print(f&quot;Loss at step {t}: {loss.item()}&quot;) layer = IncrementAutoregressive() train(layer) result = rollout(layer, 1.) print(result) . also by Ariel . State-of-the-art algorithm to print from 1 to 50 . The council consists of Ariel and me. . Another solution, inspired by the State-Of-The-Art sorting algorithm SleepSort, using advanced concurrent programming techniques: from threading import Thread import time def wait_and_print(n: int): time.sleep(n) print(n) threads = [Thread(target=wait_and_print, args=(i,)) for i in range(1, 50)] for thread in threads: thread.start() for thread in threads: thread.join() . “Let’s be language agnostic” - OrganicPitaChips: . &gt;+++++++++++[-&lt;+++++&gt;] # initialize 55??? at first cell &gt;++++++++++&lt;&lt;[-&gt;+&gt;-[&gt;+&gt;&gt;]&gt;[+[-&lt;+&gt;]&gt;+&gt;&gt;]&lt;&lt;&lt;&lt;&lt;&lt;]&gt;&gt;[-]&gt;&gt;&gt;++++++++++&lt;[-&gt;-[&gt;+&gt;&gt;]&gt;[+[- &lt;+&gt;]&gt;+&gt;&gt;]&lt;&lt;&lt;&lt;&lt;]&gt;[-]&gt;&gt;[&gt;++++++[-&lt;++++++++&gt;]&lt;.&lt;&lt;+&gt;+&gt;[-]]&lt;[&lt;[-&gt;-&lt;]++++++[-&gt;++++++++ &lt;]&gt;.[-]]&lt;&lt;++++++[-&lt;++++++++&gt;]&lt;.[-]&lt;&lt;[-&lt;+&gt;] . Final take . Thanks everyone who participated and enjoyed in this conversation! .",
            "url": "https://minhlong94.github.io/blog/shitpost/2021/09/10/A-Neural-Network-To-Print-From-1-to-50.html",
            "relUrl": "/shitpost/2021/09/10/A-Neural-Network-To-Print-From-1-to-50.html",
            "date": " • Sep 10, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Luu Minh Long (surname Luu, name Long). I am a CS undergrad student in Vietnam. Currently I am working as a Data Engineer (internship). In my free time, I participate in Kaggle competitions, and do ML research. . CV &amp; Contact information . CV: Google Docs | LinkedIn | Email: minhlong9413@gmail.com | Kaggle profile: Kaggle | GitHub: GitHub | . Research Projects . AutoMixup: Learning mix-up policies with Reinforcement Learning. Long M. Luu, Zeyi Huang, Haohan Wang. Accepted at ICML2021 ML4data Workshop. PDF | .",
          "url": "https://minhlong94.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://minhlong94.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}